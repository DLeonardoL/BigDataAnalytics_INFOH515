{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD example\n",
    "\n",
    "This notebook provide a dummy example of a map on Spark RDD, that can be used to check that parallelisation works fine on the cluster.\n",
    "\n",
    "It consist of creating an RDD with `n_partitions` partitions, and apply a map function that waits for 2 seconds for each partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: Cluster access\n",
    "\n",
    "* Connect to cluster with \n",
    "\n",
    "```\n",
    "ssh -p 30 -L 8000:jupyter:8000 -L 8888:hue:8888 -L 8088:hue:8088 yourlogin@bigdata.ulb.ac.be\n",
    "```\n",
    "\n",
    "Note the port redirection to get access locally to JupyterHub, Hue, and Hadoop Web UI\n",
    "\n",
    "* Open JupyterHub locally by connecting to `127.0.0.1:8000`\n",
    "* Upload this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "A Spark session is created by using the pyspark.sql.SparkSession object. See [here](https://spark.apache.org/docs/latest/sql-programming-guide.html#starting-point-sparksession) for the API documentation on the SparkSession Object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is needed to start a Spark session from the notebook\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g\\\n",
    "                                    pyspark-shell\"\n",
    "\n",
    "# For Yarn, so that Spark knows where it runs\n",
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "# For Yarn, so Spark knows which version to use (and we want Anaconda to be used, so we have access to numpy, pandas, and so forth)\n",
    "os.environ['PYSPARK_PYTHON']=\"/etc/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/etc/anaconda3/bin/python\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uncomment below to recreate a Spark session with other parameters\n",
    "#spark.stop()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",\"10\") \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start dummy Spark jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wait function\n",
    "def wait2s(x):\n",
    "    time.sleep(2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions=8\n",
    "\n",
    "data=range(0,n_partitions)\n",
    "datardd=sc.parallelize(data,n_partitions)\n",
    "\n",
    "datardd.map(wait2s).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Spark UI and check parallelisation\n",
    "\n",
    "* Open Hadoop Web UI at `127.0.0.1:8088`, and click on your running Application. You should land on an URL similar to `http://127.0.0.1:8088/cluster/app/application_1523870291186_0032`\n",
    "* Change `cluster/app` to `proxy` to get to the Spark UI: `http://127.0.0.1:8088/proxy/application_1523870291186_003`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability\n",
    "\n",
    "The following runs a benchmark to test the execution times of the wait2s mapping on an RDD with 100 partitions, for a number of executors increasing from 1 to 100. The mapping is repeated ten times for eah number of executors (in the set 1, 2, 5, 10, 20, 50, 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ten runs (rows), for number of executors in (1,2,5,10,20,50,100) (in columns)\n",
    "n_executors=[1,2,5,10,20,50,100]\n",
    "results_benchmark=np.zeros((10,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(n_executors)):\n",
    "    \n",
    "    print(\"Run benchmark with \"+str(n_executors[i])+\" executors\")\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",str(n_executors[i])) \\\n",
    "    .appName(\"demoRDD\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    sc=spark.sparkContext\n",
    "    \n",
    "    #100 partitions\n",
    "    n_partitions=100\n",
    "    data=range(0,n_partitions)\n",
    "    datardd=sc.parallelize(data,n_partitions)\n",
    "\n",
    "    for j in range(10):\n",
    "        time_start=time.time()\n",
    "        datardd.map(wait2s).collect()\n",
    "        time_end=time.time()\n",
    "        execution_time=time_end-time_start\n",
    "        results_benchmark[j,i]=execution_time\n",
    "    \n",
    "    spark.stop()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_results=pd.DataFrame(results_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "203.9961552619934,102.81437921524048,42.03271555900574,22.165425539016724,12.114766836166382,6.621063947677612,4.2486162185668945\n",
    "202.44642806053162,101.42299795150757,40.71962809562683,20.532904863357544,10.455049276351929,4.267818212509155,2.2575149536132812\n",
    "202.36464619636536,101.41451358795166,40.69806241989136,20.44546890258789,10.30104923248291,4.22643518447876,3.038877010345459\n",
    "202.4120738506317,101.30005168914795,40.68964910507202,20.46614170074463,10.302504062652588,4.2139832973480225,2.2561874389648438\n",
    "202.29537892341614,101.28277230262756,40.719842195510864,20.5796639919281,10.278221130371094,4.6493425369262695,2.2038023471832275\n",
    "202.13580703735352,101.29535865783691,40.626713514328,20.42010807991028,10.451300382614136,4.237480401992798,2.251011371612549\n",
    "202.09938716888428,101.22942876815796,40.62957000732422,20.397658586502075,10.503596067428589,4.172134160995483,2.301781177520752\n",
    "202.12336921691895,101.22201132774353,40.58945393562317,20.378247022628784,10.340943574905396,4.20829176902771,2.296875\n",
    "202.11851453781128,101.29037380218506,40.55907702445984,20.389169216156006,10.39184284210205,4.261171340942383,2.4329993724823\n",
    "202.07814502716064,101.16865134239197,40.75089430809021,20.365695476531982,10.275054216384888,4.304711818695068,2.1503186225891113\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_results.to_csv(\"resultsBenchmark.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
